import os
import json
import time
import sqlite3
from datetime import datetime
from pathlib import Path

import pandas as pd
import streamlit as st
from streamlit_autorefresh import st_autorefresh  # pip install streamlit-autorefresh

# -----------------------------
# Config: ë¡œì»¬ ê²½ë¡œ(í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
# -----------------------------
DATA_DIR = Path(os.getenv("HVDC_DATA_DIR", r"C:\hvdc\data"))
QUEUE_FILE = Path(os.getenv("HVDC_RETRY_QUEUE", str(DATA_DIR / "retry_queue.jsonl")))
LOG_DIR = Path(os.getenv("HVDC_WHATSAPP_LOG_DIR", str(DATA_DIR / "whatsapp_logs")))
KPI_CSV = Path(os.getenv("HVDC_KPI_CSV", str(DATA_DIR / "kpi_report.csv")))
KPI_SQLITE = Path(os.getenv("HVDC_KPI_SQLITE", str(DATA_DIR / "logs.sqlite")))

REFRESH_SECS = int(os.getenv("HVDC_DASHBOARD_REFRESH", "10"))  # ìë™ ìƒˆë¡œê³ ì¹¨ ê°„ê²©(ì´ˆ)

st.set_page_config(page_title="HVDC Local Dashboard", layout="wide")

# ìë™ ìƒˆë¡œê³ ì¹¨(í´ë¦°í•˜ê²Œ ì¬ì‹¤í–‰)
st_autorefresh(interval=REFRESH_SECS * 1000, key="auto_refresh")

st.title("âš¡ HVDC Local Dashboard")
st.caption(f"DATA_DIR: {DATA_DIR}")

# --------------------------------
# Helpers
# --------------------------------
def _read_jsonl(path: Path, nrows: int | None = None) -> pd.DataFrame:
    """JSON Lines íŒŒì¼ -> DataFrame (lines=True)"""
    if not path.exists():
        return pd.DataFrame()
    # pandas.read_json(lines=True) ì‚¬ìš© ê¶Œì¥. :contentReference[oaicite:2]{index=2}
    try:
        return pd.read_json(path, lines=True) if nrows is None else pd.read_json(path, lines=True, nrows=nrows)
    except ValueError:
        # í¬ë§· ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë¹ˆ DF
        return pd.DataFrame()

def _read_latest_appendlogs(log_dir: Path, limit: int = 50) -> pd.DataFrame:
    if not log_dir.exists():
        return pd.DataFrame()
    rows = []
    for p in sorted(log_dir.glob("*.json"), reverse=True)[:limit]:
        try:
            with open(p, "r", encoding="utf-8") as f:
                obj = json.load(f)
            obj["_file"] = str(p)
            rows.append(obj)
        except Exception:
            continue
    return pd.DataFrame(rows)

# ìºì‹œëœ ë¡œë”ë“¤(ë°ì´í„° ë°˜í™˜ ìºì‹±). :contentReference[oaicite:3]{index=3}
@st.cache_data(ttl=5)  # ì†Œê·œëª¨ íŒŒì¼ì€ 5ì´ˆ ìºì‹œ
def load_queue_df() -> pd.DataFrame:
    return _read_jsonl(QUEUE_FILE)

@st.cache_data(ttl=5)
def load_logs_df(limit: int = 100) -> pd.DataFrame:
    return _read_latest_appendlogs(LOG_DIR, limit=limit)

@st.cache_data(ttl=10)
def load_kpi_csv() -> pd.DataFrame:
    if KPI_CSV.exists():
        try:
            return pd.read_csv(KPI_CSV)
        except Exception:
            pass
    return pd.DataFrame()

@st.cache_data(ttl=10)
def load_kpi_sqlite() -> pd.DataFrame:
    if not KPI_SQLITE.exists():
        return pd.DataFrame()
    # sqlite3.connectë¡œ ì¡°íšŒ(í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬). :contentReference[oaicite:4]{index=4}
    con = sqlite3.connect(str(KPI_SQLITE))
    try:
        # ì‚¬ìš© ì¤‘ì¸ KPI ë·°/í…Œì´ë¸” ì´ë¦„ì— ë§ì¶° ì¡°ì •í•˜ì„¸ìš”.
        # ì˜ˆ: v_kpi_daily(date, group_name, logs, sla_breaches)
        return pd.read_sql_query("SELECT * FROM v_kpi_daily", con)
    except Exception:
        try:
            # í´ë°±: ëŒ€ëµì ì¸ êµ¬ì¡° ì¶”ì •
            return pd.read_sql_query("SELECT * FROM kpi_daily", con)
        except Exception:
            return pd.DataFrame()
    finally:
        con.close()

# -----------------------------
# Layout
# -----------------------------
colA, colB, colC = st.columns(3)

# Queue KPIs
qdf = load_queue_df()
queued = int((qdf["status"] == "queued").sum()) if not qdf.empty else 0
retrying = int((qdf["status"] == "retrying").sum()) if not qdf.empty else 0
dead = int((qdf["status"] == "deadletter").sum()) if not qdf.empty else 0
done = int((qdf["status"] == "done").sum()) if not qdf.empty else 0

colA.metric("Queued", queued)
colB.metric("Retrying", retrying)
colC.metric("Deadletter", dead)

st.divider()

# ì¢Œ: ìµœì‹  appendLog / ìš°: KPI
left, right = st.columns([1.2, 1])

with left:
    st.subheader("ğŸ“ Latest appendLog files")
    ldf = load_logs_df(limit=200)
    if ldf.empty:
        st.info(f"No JSON files under {LOG_DIR}")
    else:
        # ì£¼ìš” ì—´ ì •ë¦¬
        cols = [c for c in ["date_gst","group_name","summary","top_keywords","sla_breaches","_file"] if c in ldf.columns]
        st.dataframe(ldf[cols], use_container_width=True, hide_index=True)
        st.caption(f"{len(ldf)} rows Â· dir={LOG_DIR}")

with right:
    st.subheader("ğŸ“ˆ KPI Overview")
    k1 = load_kpi_csv()
    k2 = load_kpi_sqlite()

    tab1, tab2 = st.tabs(["KPI (CSV)", "KPI (SQLite)"])
    with tab1:
        if k1.empty:
            st.warning(f"No CSV at {KPI_CSV}")
        else:
            st.dataframe(k1, use_container_width=True, hide_index=True)
            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.download_button("Download KPI CSV", data=k1.to_csv(index=False).encode("utf-8"),
                               file_name="kpi_export.csv", mime="text/csv")

    with tab2:
        if k2.empty:
            st.warning(f"No SQLite KPI at {KPI_SQLITE}")
        else:
            st.dataframe(k2, use_container_width=True, hide_index=True)

st.divider()

# Queue raw table(ìµœê·¼ Ní–‰)
st.subheader("ğŸ” Retry Queue (recent)")
if qdf.empty:
    st.info(f"No queue file at {QUEUE_FILE}")
else:
    # ìµœì‹ ìˆœìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°(íŒŒì¼ì„ ì—­ìˆœ ìŠ¤ìº”í•˜ì§€ ëª»í–ˆì„ ë•Œ ëŒ€ë¹„)
    view = qdf.copy()
    # ì‹œê°„ ë¬¸ìì—´ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì •ë ¬
    for tcol in ["next_try_at_gst","enqueued_at_gst"]:
        if tcol in view.columns:
            # ì •ë ¬ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
            try:
                view[tcol] = pd.to_datetime(view[tcol], errors="coerce", format="%Y-%m-%d %H:%M")
            except Exception:
                pass
    # ë³´ì—¬ì¤„ ì—´ ì¶”ë¦¼
    show_cols = [c for c in ["type","status","attempt","max_attempts","next_try_at_gst","last_error","idempotency_key"] if c in view.columns]
    view = view.sort_values(by=[c for c in ["next_try_at_gst","enqueued_at_gst"] if c in view.columns], ascending=False)
    st.dataframe(view[show_cols], use_container_width=True, hide_index=True)

# Footer
st.caption(f"Auto-refresh: every {REFRESH_SECS}s Â· Now: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
